
Algoritmos de Gran Escala:  
Trabajo Final
========================================================
author: Andrea Fernández, Andrea García y Mario Becerra
date: 25/05/15

Elegir una base de datos para clasificación
========================================================

Para la elaboración de este trabajo se eligió una base de declaratorias de desastres de SEGOB. La variable dependiente toma el valor de 1 si fue declarado (desastre, contingencia climática o  emergencia), 0 si no fue declarado. 

Para las covariabes utilizamos variables geográficas (como nivel de riesgo a inundación, deslave, huracán y  sequía) y variables socioeconómicas tales como tasa de alfabetización, nivel de hacinamiento, tamaño de la locaidad, entre otras. 


EDA
========================================================

La distribución por Grado de Marginación nos muestra que los grados altos tienen más declaratorias. 

![](./img/dec_GM.png)


EDA
========================================================

En cuanto al tipo de fenómeno  la mayor parte de las declaratorias se concentran en lluvias y sequías.


![](./img/tipo.png)



Adaptar el código de SVM para el clúster 
========================================================


Para el Cluster actualizamos las versiones de R y snow para las 2 computadoras del Laboratorio de Ciencia d Datos. 
Ya no utilizamos MPI dado que los comandos en la ultima versión cambiaron y  después de intentarlo por varios días decidimos solo usar snow 



Puntos Interiores 
========================================================


En el script de "solsistparalelo.R" en la función MakeCluster se cambia MakeSOCKcluster donde esta el master y el escalvo. 



Regresión Logística
========================================================
```{r, echo=FALSE}
library(glmnet)
library(caret)
library(knitr)
load('./data/mod.1.RData')
train <- read.csv('./data/desastres_train.csv', colClasses=c('factor', rep('numeric', 18)))
test <- read.csv('./data/desastres_test.csv', colClasses=c('factor', rep('numeric', 18)))
predicted.glm <- predict(mod.1, as.matrix(test[,-1]), type="response")
predicted.class <- as.factor(predicted.glm>0.5)
levels(predicted.class) <- c(0,1)
cm <- confusionMatrix(predicted.class, test$Dependiente)
kable(cm$table)
```


Problemas 
========================================================

**Implementación del cluster**

El principal problema fue que no entendiamos cómo funcionaba MPI, además la última versión de MPI cambió sus comandos 

![](./img/esquema_mpi.png)


Paralelización  
========================================================

**SNOW**

Al final preferimos usar SNOW en lugar de Open_MPI

![](./img/paralelo.jpg)
