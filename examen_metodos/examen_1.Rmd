---
title: "Examen 1, Métodos análiticos"
output: html_document
---

Entrega tus resultados y el código que utilizaste. 

### Parte 1: correos de Enron, similitud y minhashing.

En este ejemplo construiremos uan aplicación para devolver rápidamente
correos similares a uno dado, en el sentido de que contienen palabras similares. Utilizaremos minhashing/LSH.

Utilizaremos los datos de correo de Enron de https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

El formato está explicado  en el archivo que acompaña los datos:

he format of the docword.*.txt file is 3 header lines, followed by 
NNZ triples: 
--- 
D 
W 
NNZ 
docID wordID count 
docID wordID count 
docID wordID count 
docID wordID count 
... 
docID wordID count 
docID wordID count 
docID wordID count 
--- 


### Enron (bag of words)

```{r}
library(dplyr)
enron <- read.table('enron/docword.enron.txt', skip=3, header=FALSE, sep=' ')
names(enron) <- c('doc','word_id','n')
head(enron)
length(unique(enron$word_id))
length(unique(enron$doc))
vocab <- read.table('enron/vocab.enron.txt', header=FALSE)
vocab$word_id <- 1:nrow(vocab)
```

Usaremos **similitud de jaccard** basada en el modelo *bag of words* para documentos, es decir, sólo en función de las palabras que contienen: la similitud entre el documento A y B es el número de palabras en común dividido entre el número total de palabras que ocurren en los dos documentos. Aquí hay una implementación simple:


```{r}
jaccard <- function(doc1, doc2){
  a <- length(union(doc1$word_id, doc2$word_id))
  c <- length(intersect(doc1$word_id, doc2$word_id))
  c/a
}
jaccard(filter(enron, doc==1), filter(enron, doc==2))
jaccard(filter(enron, doc==1), filter(enron, doc==8000))
```

1. Construye una matriz de firmas de minhashing para esta colección. Utiliza la matriz de firmas para encontrar mails similares al 900 (más de 50% de similitud de Jaccard) ¿Qué palabras comparten estos documentos?

2. (LSH) Utiliza 8 bandas de 2 hashes cada una para obtener pares candidatos para similitud. Devuelve los 20 mejores candidatos (si existen) para los documentos 100, 105,1400. Recuerda calcular la similitud exacta para los pares candidatos que consideres. 

3. Describe la distribución de palabras para cada uno de los tres clusters que encontraste en el inciso anterior. 
Describe los clusters en términos de las palabras más comunes. Utiliza código como el siguiente:

```{r}
filter(enron, doc %in% c(900, 1153, 1506, 2083, 2325, 2435)) %>% group_by(word_id) %>%
  summarise(n_tot= sum(n)) %>% arrange(desc(n_tot)) %>% left_join(vocab)
```

Ayuda: puedes comenzar con el siguiente código si quieres

```{r}
set.seed(2805)
hash.lista <- lapply(1:200, function(i){
    primo <- 28099
    a <- sample(1:(primo-1), 1)
    b <- sample(1:(primo-1), 1)
    function(x){
        ((a*(x-1) + b) %% primo)  + 1
    }
})

minhash <- function(dat, hash.lista){
    n_words <- 28099
    n_docs <- 39861
    p <- length(hash.lista)
    sig <- matrix(rep(Inf, p*n_docs) , ncol = n_docs)
    for(i in 1:n_words){
      #######################################
      
      ##### puedes rellenar tu código aquí
      
      #######################################
    }
    sig
}
```

### Parte 2: Aplicación a búsqueda de items populares

Implementa la aplicación que vimos de búsqueda de items populares. Muestra
las 15 películas más populares en enero de 2000, junio de 2000 y enero de 2001
Puedes utilizar los datos proporcionados en el sitio de la clase para evitar hacer más procesamiento.


### Parte 3: Recomendación


https://movielens.org

Utilizaremos datos de movielens:

These files contain 1,000,209 anonymous ratings of approximately 3,900 movies 
made by 6,040 MovieLens users who joined MovieLens in 2000.


```{r}
rat <- read.table('ml-1m/ratings.dat',header=F, sep=":")
rat$V2 <- NULL
rat$V4 <- NULL
rat$V6 <- NULL
head(rat)
names(rat) <- c('user_id','movie_id','rating')
rat.2 <- rat %>% group_by(user_id) %>%
  mutate(media.usu = mean(rating)) %>%
  group_by(movie_id) %>%
  mutate(media.movie = mean(rating)) %>%
  ungroup() %>%
  mutate(media = mean(rating)) %>%
  mutate(rating.adj = rating - (media.movie + media.usu - media))
library(Matrix)
i <- rat.2$user_id
j <- rat.2$movie_id
x <- rat.2$rating.adj
X <- sparseMatrix(i=i,j=j,x=x)
```


```{r}
con <- file("ml-1m/movies.dat", "r", blocking = FALSE)
lineas <- readLines(con) # empty
close(con)
lista.movies <- list()
salida <- lapply(lineas, function(linea){
  sp <- strsplit(linea, '::', fixed=T)[[1]]
  data_frame(movie_id = sp[1], movie_nom = sp[2], tipo = sp[3])
})
movies.df <- rbind_all(salida)
movies.df$movie_id <- as.integer(movies.df$movie_id)
```

1. Construye una muestra de entrenamiento y una de validación
2. Ajusta y evalúa el modelo base.
3. Utiliza descenso estocástico o descomposición en valores singulares para encontrar factores latentes (ajustados a los residuales del modelo base). 
4. Explica cómo hacer predicciones a partir del modelo (predicción de la calificación 1-5). ¿Qué películas recomendarías para el usuario usuario 4000 y el usuario 6000, y usuario 1333? (que no haya visto!)
5. Evalúa el modelo de factores latentes que ajustaste usando la muestra de validación.

Nota: puedes intentar usar el código de clase para descenso en gradiente. También puedes usar este código simple que hace descomposición en valores singulares:

```{r}
library("irlba")
out <- irlba(X, nu=50, nv=50) # 50 factores latentes con svd, 
factores.peliculas <- data.frame(out$v%*%diag(sqrt(out$d)), movie_id=1:nrow(out$v)) %>% 
  left_join(movies.df) %>%
  arrange(desc(X2))
factores.personas <- data.frame(out$u%*%diag(sqrt(out$d)), user_id=1:nrow(out$u))
```




