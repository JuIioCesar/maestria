---
title: "Examen 1, Métodos análiticos"
output: html_document
---

Entrega tus resultados y el código que utilizaste. 

# Parte 1: correos de Enron, similitud y minhashing.

En este ejemplo construiremos uan aplicación para devolver rápidamente
correos similares a uno dado, en el sentido de que contienen palabras similares. Utilizaremos minhashing/LSH.

Utilizaremos los datos de correo de Enron de https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

El formato está explicado  en el archivo que acompaña los datos:

he format of the docword.*.txt file is 3 header lines, followed by 
NNZ triples: 
--- 
D 
W 
NNZ 
docID wordID count 
docID wordID count 
docID wordID count 
docID wordID count 
... 
docID wordID count 
docID wordID count 
docID wordID count 
--- 


### Enron (bag of words)

```{r}
# lectura de datos
library(dplyr)
library(Rcpp)
enron <- read.table('enron/docword.enron.txt', skip=3, header=FALSE, sep=' ')
names(enron) <- c('doc','word_id','n')
head(enron)
length(unique(enron$word_id))
length(unique(enron$doc))
vocab <- read.table('enron/vocab.enron.txt', header=FALSE)
vocab$word_id <- 1:nrow(vocab)
names(vocab) <- c("word", "word_id")
```

Usaremos **similitud de jaccard** basada en el modelo *bag of words* para documentos, es decir, sólo en función de las palabras que contienen: la similitud entre el documento A y B es el número de palabras en común dividido entre el número total de palabras que ocurren en los dos documentos. Aquí hay una implementación simple:


```{r}
jaccard <- function(doc1, doc2){
  a <- length(union(doc1$word_id, doc2$word_id))
  c <- length(intersect(doc1$word_id, doc2$word_id))
  c/a # y las repetidas? sets...
}
jaccard(filter(enron, doc==1), filter(enron, doc==2))
jaccard(filter(enron, doc==1), filter(enron, doc==8000))
```

## Minhashing
Construye una matriz de firmas de minhashing para esta colección. Utiliza la matriz de firmas para encontrar mails similares al 900 (más de 50% de similitud de Jaccard) ¿Qué palabras comparten estos documentos?

Necesitamos generar $m$ funciones hash tales que
\[
h_i(x) = (ax + b mod p) mod k
\]

donde $k$ es el número de tejas posibles, $p$ un primo tal que $p > k > 0$, $a,b$ escogidos al azar donde $0<a$ y $a,b<p$. 

Para generar una lista de hashes con estas características, utilizamos la siguiente función.
```{r}
# # Mi funcion
# set.seed(2805)
# m <- 200 # numero de funciones hash
# hash.lista <- lapply(1:m, function(i){
#     p <- 29167# primo > 28099 (numero de tejas)
#     a <- sample(1:(p-1), 1) # 0<a<p
#     b <- sample(1:(p-1), 1) # 0<b<p
#     function(x){
#         (((a*(x-1) + b) %% p) %% 28099) + 1
#     }
# })

# La de la ayuda...
set.seed(2805)
hash.lista <- lapply(1:200, function(i){
    primo <- length(unique(enron$word_id))
    a <- sample(1:(primo-1), 1)
    b <- sample(1:(primo-1), 1)
    function(x){
        ((a*(x-1) + b) %% primo)  + 1
    }
})
```

Utilizaremos el algoritmo por renglón para poder calcular la matriz de firmas. 

Para este debemos seguir los siguientes pasos:

1. Definimos la matriz de firmas como $sig(i,c)=inf$.
2. Ciclamos
    - Para cada renglón $r$ (_teja_):
        - Para cada documento $c$ (_columna_):
            - Si $c$ tiene un 1 en el renglón $r$, entonces para cada función hash $h_i$, si $h_i(r)$ es menor a $sig(i,c)$, entonces $sig(i,c)=h_i(r)$.
            
Al final del algoritmo, $sig$ es la matriz de firmas de los documentos bajo las funciones hash $h_1,...,h_m$.

Implementamos el algoritmo de tal forma que aprovechemos que la matriz de tejas documentos ya está calculada en _enron_.

```{r, eval=F}
# Jalamos funcion auxiliar
sourceCpp('src/update_mat.cpp')

minhash <- function(matriz, hashes){
    r <- length(unique(enron$word_id))  # numero de palabras
    c <- length(unique(enron$doc)) # numero de documentos
    m <- length(hashes) # numero de hashes
    sig <- matrix(rep(Inf, m*c), ncol=c) # Paso 1 algoritmo
    for(i in 1:r){ # Paso 2: iteramos en renglones
        renglon <- sapply(hashes, function(hash){hash(i)})# Aplicamos todos los hashes a cada palabra en el vocabulario
        docs <- enron$doc[which(enron$word_id==i)]# Extraemos todos los docs con esa palabra
        out <- sapply(docs,function(doc){update_mat(sig, doc, renglon)})# Updateamos la matriz de firmas iterando sobre los docs que tienen esa palabra
    }
    sig
}
    
firmas <- minhash(enron, hash.lista)
save(firmas, file='./output/firmas.Rdata')
```

Par de sanity checks.

```{r}
load('output/firmas.Rdata')
dim(firmas)
firmas[1:6,1:6]
```

Una vez calculada la matriz de firmas, encontramos los mails más parecidos al 900 con similitud de jaccard mayor al 50\%.

```{r}
doc <- 900
# Lo vemos
words <- left_join(filter(enron, doc==900),vocab)$word #correo grosero!
words
```

```{r}
# extraemos sus firmas
doc.firma <- firmas[,doc]
# sacamos similitudes aproximadas
sim_est <- apply(firmas, 2, function(x){mean(x==doc.firma)})
# sacamos docs con similitud mayor a 50%
docs.sim <- which(sim_est>=0.5)

# palabras compartidas
lapply(docs.sim[-1], FUN=function(x){union(left_join(filter(enron, doc==900),vocab)$word, left_join(filter(enron, doc==x),vocab)$word)})
```

## LSH
Utiliza 8 bandas de 2 hashes cada una para obtener pares candidatos para similitud. Devuelve los 20 mejores candidatos (si existen) para los documentos 100, 105,1400. Recuerda calcular la similitud exacta para los pares candidatos que consideres. 

En el gráfico siguiente, se observa la probabilidad de que al menos coincida una banda de minhashes en función de la similitud.

```{r}
r=2
b=8
plot(s <- seq(0, 1, by = 0.01), 1 - (1 - s^r)^b, type = "l", ylab = "Probabilidad al menos una banda",
     xlab = "Similitud (jaccard)")
```

Ahora dividimos la matriz de firmas en 8 bandas de 2 hashes cada uno. Después, calculamos un nuevo hash para extraer pares candidatos. Obtenemos una matriz de dimensión documentos por hashes.

```{r, warning=F}
mat.split <- split(data.frame((firmas)), rep(1:8, each=2))

# Agarro la firma y la sumo - asi tal cual! Ese es mi hash ahora. Obviamente, con la suma hay falsos positivos pero es muy eficiente. Si la suma es igual, digo que son paress candidatos (es decir, que caen en "la misma cubeta")
mat.hashed <- sapply(mat.split, function(mat){
    apply(mat, 2, function(x){   sum(x) })
})
dim(mat.hashed) # documentos x (hashes de la banda)

# Le agregamos el numero del documento 
mat.hashed <- data.frame(mat.hashed)
mat.hashed$doc_no <- 1:nrow(mat.hashed)
```

Extraemos los pares que coinciden en la primera banda.

```{r, eval=T}
# Calculamos pares candidatos
candidatos <- lapply(1:8, function(i){ #para cada banda
    tab.1 <- table(mat.hashed[,i]) # extraemos el hash de la banda i
    codigos <- as.integer(names(tab.1[tab.1 >= 2])) 
   salida <- lapply(codigos, function(cod){ 
        mat.hashed$doc_no[mat.hashed[,i] == cod]
    })
   Reduce('cbind',lapply(salida, function(i){combn(i,2)}))
})
candidatos.tot <- t(Reduce('cbind',candidatos))
head(candidatos.tot)

# Dimension de los candidatos calculados
dim(candidatos.tot)

candidatos.tot.2 <- unique(candidatos.tot)
dim(candidatos.tot.2)

save(candidatos.tot.2, candidatos.tot, file="output/candidatos.Rdata")

```
Los pares candidatos son `r dim(candidatos.tot.2)[1]` y están guardados en _candidatos.tot.2_. Ahora debemos calcular la similitud exacta para cada uno de esos pares para eliminar falsos positivos.

Como vemos, son muchos los pares candidatos. Nos quedamos únicamente con los de interes y eliminamos de la matriz filtrada la coicidencia consigo mismo.

```{r}
load('./output/candidatos.Rdata')
# Nos quedamos los pares candidatos que incluyen los de interes
interes <- c(100,105,1400)
todos <- as.data.frame(candidatos.tot.2)
docs.interes <- todos[which(todos$V1 %in% interes | todos$V2 %in% interes),] %>% arrange(V1)

# Calculamos Jaccard exacto pa los candidatos
docs.interes$jaccard <- apply(docs.interes, MARGIN=1, FUN=function(x){
    jaccard(filter(enron, doc==x[1]), filter(enron, doc==x[2]))
})


# 
por.doc <- lapply(interes, function(x){filter(docs.interes, V1==x | V2==x)})
# Los primeros 20 documentos similares, en orden son
lapply(por.doc, function(df){
    simil <- df %>% arrange(-jaccard) %>% select(V2)
    simil[!is.na(simil[1:20,]),]
})


```

## LSH
Describe la distribución de palabras para cada uno de los tres clusters que encontraste en el inciso anterior. 
Describe los clusters en términos de las palabras más comunes. Utiliza código como el siguiente:

```{r}
docs.similares <- lapply(por.doc, function(df){
    simil <- df %>% arrange(-jaccard) %>% select(V2) 
    simil$V2
})

lapply(docs.similares, FUN=function(docs){filter(enron, doc %in% docs) %>% group_by(word_id) %>%
  summarise(n_tot= sum(n)) %>% arrange(desc(n_tot)) %>% left_join(vocab)})
```


__Nota__ Se puede agregar un wordcloud para facilitar la interpretacion x cluster. Tambien se puede hacer unos graficos de barras o cosas asi.

### 

Ayuda: puedes comenzar con el siguiente código si quieres

```{r, eval=F}
set.seed(2805)
hash.lista <- lapply(1:200, function(i){
    primo <- 28099
    a <- sample(1:(primo-1), 1)
    b <- sample(1:(primo-1), 1)
    function(x){
        ((a*(x-1) + b) %% primo)  + 1
    }
})

minhash <- function(dat, hash.lista){
    n_words <- 28099
    n_docs <- 39861
    p <- length(hash.lista)
    sig <- matrix(rep(Inf, p*n_docs) , ncol = n_docs)
    for(i in 1:n_words){
      #######################################
      
      ##### puedes rellenar tu código aquí
      
      #######################################
    }
    sig
}
```

